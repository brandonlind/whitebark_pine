{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "from collections import OrderedDict,Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vcf\n",
    "from operator import itemgetter\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.ticker as mtick\n",
    "%matplotlib inline\n",
    "import math\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.stats import anderson_ksamp\n",
    "from os import listdir as ls\n",
    "from os import path as op\n",
    "from scipy.stats import spearmanr\n",
    "import skbio\n",
    "from scipy.stats import pearsonr\n",
    "import shutil\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get the hierftrans for all SNPS\n",
    "#z12 file created in 06_pca.ipyn\n",
    "filE = '/home/lindb/wbp/OutFLANK/imputed_z12_maf_swp_trans_z12.txt'\n",
    "imp012 = pd.read_csv(filE,header=0,index_col=0,sep=\"\\t\")\n",
    "imp012.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get pop assignment for each samp\n",
    "filE = '/home/lindb/wbp/sampsTOpop.txt'\n",
    "stp = pd.read_csv(filE,header=0,index_col='sampID',sep=\"\\t\")\n",
    "stp.sort_index(inplace=True)\n",
    "stp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#list of samps per pop\n",
    "stpdict= OrderedDict()\n",
    "for row in stp.index:\n",
    "    pop = stp.loc[row,'pop']\n",
    "    if not pop in stpdict.keys():\n",
    "        stpdict[pop] = []\n",
    "    stpdict[pop].append(row)\n",
    "stpdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#counts per pop\n",
    "popdict = Counter()\n",
    "for row in stp.index:\n",
    "    pop = stp.loc[row,'pop']\n",
    "    if not pop in popdict.keys():\n",
    "        popdict[pop] = 0\n",
    "    popdict[pop] += 1\n",
    "popdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samps = stp.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reassign individuals to a random population\n",
    "randpopdict = OrderedDict()\n",
    "for pop in popdict:\n",
    "    randpopdict[pop] = []\n",
    "    randpopdict[pop] = random.sample(samps,popdict[pop])\n",
    "    samps = set(samps) - set(randpopdict[pop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make sure all samples were assigned to pops\n",
    "sums = 0\n",
    "for pop in randpopdict:\n",
    "    sums = sums + len(randpopdict[pop])\n",
    "sums == sum(popdict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make sure pop size is the same\n",
    "for pop in randpopdict:\n",
    "    print pop,len(randpopdict[pop]),popdict[pop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#see how many of the individuals stayed in the same pop\n",
    "for pop in popdict:\n",
    "    print pop, len(set(randpopdict[pop]).intersection(set(stpdict[pop])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make new stp dataframe\n",
    "stpnew = pd.DataFrame(stp)\n",
    "for samp in stpnew.index:\n",
    "    stpnew.loc[samp,'pop'] = [pop for pop,samplist in randpopdict.items() if samp in samplist][0]\n",
    "stpnew.sort_index(inplace=True)\n",
    "stpnew.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged = pd.merge(imp012,stpnew,left_index=True,right_index=True)\n",
    "cols = ['pop'] + [col for col in merged.columns if 'NODE' in col]\n",
    "merged = merged[cols]\n",
    "merged.sort_index(inplace=True)\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = [col for col in merged.columns if 'NODE' in col]\n",
    "snpmat = merged[cols]\n",
    "snpmat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pops = pd.DataFrame(merged['pop'].tolist())\n",
    "pops.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = \"/home/lindb/wbp/outflank_null\"\n",
    "if not op.exists(DIR):\n",
    "    os.makedirs(DIR)\n",
    "    print \"True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/outflank_null/SNPmat_HEADERIDX.txt'\n",
    "filE2 = '/home/lindb/wbp/outflank_null/SNPmat_noHEADERIDX.txt'\n",
    "print 'making 1st snpmat' #so I don't have to watch ls -lt\n",
    "snpmat.to_csv(filE,header=True,index=True,sep=\"\\t\")\n",
    "print 'making 2nd snpmat'\n",
    "snpmat.to_csv(filE2,header=None,index=False,sep=\"\\t\")\n",
    "\n",
    "popfile = '/home/lindb/wbp/outflank_null/SNPmat_popNames.txt'\n",
    "print 'making popfile'\n",
    "pops.to_csv(popfile,header=False,index=False,sep=\"\\t\")\n",
    "\n",
    "print 'making locfile'\n",
    "locfile = '/home/lindb/wbp/outflank_null/SNPmat_locusNames.txt'\n",
    "cols = pd.DataFrame(snpmat.columns)\n",
    "cols.to_csv(locfile,header=False,index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = '''\n",
    "library(OutFLANK)\n",
    "library(data.table)\n",
    "\n",
    "SNPmat = data.frame(fread('/home/lindb/wbp/outflank_null/SNPmat_noHEADERIDX.txt',header=F,sep=\"\\\\t\"))\n",
    "\n",
    "locusNames = read.csv('/home/lindb/wbp/outflank_null/SNPmat_locusNames.txt',header=F,sep=\"\\\\t\")\n",
    "\n",
    "popNames = read.csv('/home/lindb/wbp/outflank_null/SNPmat_popNames.txt',header=F,sep=\"\\\\t\")\n",
    "\n",
    "FstDataFrame = MakeDiploidFSTMat(SNPmat,locusNames,popNames)\n",
    "\n",
    "out = OutFLANK(FstDataFrame = FstDataFrame,NumberOfSamples = 8)\n",
    "\n",
    "df = out$results\n",
    "\n",
    "outliers = df[which(df$OutlierFlag == 'TRUE'),]\n",
    "\n",
    "loci = outliers$LocusName\n",
    "\n",
    "write.table(df,'/home/lindb/wbp/outflank_null/OutFLANK_results.txt',row.names=F,col.names=T,sep='\\\\t')\n",
    "\n",
    "write.table(loci,'/home/lindb/wbp/outflank_null/OutFLANK_snps.txt',row.names=F,sep='\\\\t')\n",
    "\n",
    "print(\"DONE!\")\n",
    "'''\n",
    "filE = \"/home/lindb/wbp/outflank_null/do_the_outflank.R\"\n",
    "with open(filE,'w') as o:\n",
    "    o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tmux running on godel35\n",
    "#sourced the R script, waiting to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loc = pd.read_csv(\"/home/lindb/wbp/outflank_null/OutFLANK_snps.txt\",sep=\"\\t\")\n",
    "loc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(loc.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "locold = pd.read_csv(\"/home/lindb/wbp/OutFLANK/OutFLANK_snps.txt\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(set(loc['x']).intersection(set(locold['x'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Make other 99 runs of null outflank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(99):\n",
    "    DIR = \"/home/lindb/wbp/outflank_null/null_%s\" % (str(i+2)).zfill(3)\n",
    "    if not op.exists(DIR):\n",
    "        os.makedirs(DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time z12 = pd.read_csv('/home/lindb/wbp/OutFLANK/imputed_z12_maf_swp_trans_z12.txt',header=0,index_col=0,sep=\"\\t\")\n",
    "filE = '/home/lindb/wbp/OutFLANK/imputed_z12_maf_swp_trans_z12.pkl'\n",
    "with open(filE,'wb') as o:\n",
    "    pickle.dump(z12,o,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(99):\n",
    "    text = '''\n",
    "from __future__ import division\n",
    "import os\n",
    "from collections import OrderedDict,Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vcf\n",
    "from operator import itemgetter\n",
    "import random\n",
    "import math\n",
    "from os import listdir as ls\n",
    "from os import path as op\n",
    "import pickle\n",
    "\n",
    "DIR = \"/home/lindb/wbp/outflank_null/null_%%s\" %% (str(%s)).zfill(3)\n",
    "print DIR\n",
    "\n",
    "\n",
    "#get the hierftrans for all SNPS\n",
    "#z12 file created in 06_pca.ipyn\n",
    "filE = '/home/lindb/wbp/OutFLANK/imputed_z12_maf_swp_trans_z12.pkl'\n",
    "imp012 = pickle.load(open(filE,'rb'))\n",
    "print \"loaded imp\"\n",
    "\n",
    "#get pop assignment for each samp\n",
    "filE = '/home/lindb/wbp/sampsTOpop.txt'\n",
    "stp = pd.read_csv(filE,header=0,index_col='sampID',sep=\"\\\\t\")\n",
    "stp.sort_index(inplace=True)\n",
    "print \"made stp df\"\n",
    "\n",
    "#list of samps per pop\n",
    "stpdict= OrderedDict()\n",
    "for row in stp.index:\n",
    "    pop = stp.loc[row,'pop']\n",
    "    if not pop in stpdict.keys():\n",
    "        stpdict[pop] = []\n",
    "    stpdict[pop].append(row)\n",
    "print \"made stpdict\"\n",
    "\n",
    "#counts per pop\n",
    "popdict = Counter()\n",
    "for row in stp.index:\n",
    "    pop = stp.loc[row,'pop']\n",
    "    if not pop in popdict.keys():\n",
    "        popdict[pop] = 0\n",
    "    popdict[pop] += 1\n",
    "print \"made popdict\"\n",
    "\n",
    "samps = stp.index.tolist()\n",
    "\n",
    "#reassign individuals to a random population\n",
    "randpopdict = OrderedDict()\n",
    "for pop in popdict:\n",
    "    randpopdict[pop] = []\n",
    "    randpopdict[pop] = random.sample(samps,popdict[pop])\n",
    "    samps = set(samps) - set(randpopdict[pop])\n",
    "print \"reassigned individuals to random pop\"\n",
    "\n",
    "#make sure all samples were assigned to pops\n",
    "sums = 0\n",
    "for pop in randpopdict:\n",
    "    sums = sums + len(randpopdict[pop])\n",
    "assert sums == sum(popdict.values())\n",
    "\n",
    "#make sure pop size is the same\n",
    "for pop in randpopdict:\n",
    "    assert len(randpopdict[pop]) == popdict[pop]\n",
    "\n",
    "#make new stp dataframe\n",
    "stpnew = pd.DataFrame(stp)\n",
    "for samp in stpnew.index:\n",
    "    stpnew.loc[samp,'pop'] = [pop for pop,samplist in randpopdict.items() if samp in samplist][0]\n",
    "stpnew.sort_index(inplace=True)\n",
    "print \"made stpnew\"\n",
    "\n",
    "#merge the dataframes\n",
    "merged = pd.merge(imp012,stpnew,left_index=True,right_index=True)\n",
    "cols = ['pop'] + [col for col in merged.columns if 'NODE' in col]\n",
    "merged = merged[cols]\n",
    "merged.sort_index(inplace=True)\n",
    "print \"merged\"\n",
    "\n",
    "#make snpmat\n",
    "cols = [col for col in merged.columns if 'NODE' in col]\n",
    "snpmat = merged[cols]\n",
    "\n",
    "pops = pd.DataFrame(merged['pop'].tolist())\n",
    "\n",
    "filE = op.join(DIR,'SNPmat_HEADERIDX_%s.txt')\n",
    "filE2 = op.join(DIR,'SNPmat_noHEADERIDX_%s.txt')\n",
    "\n",
    "snpmat.to_csv(filE,header=True,index=True,sep=\"\\\\t\")\n",
    "print \"made snpmat\"\n",
    "\n",
    "snpmat.to_csv(filE2,header=None,index=False,sep=\"\\\\t\")\n",
    "print \"made second snpmat\"\n",
    "\n",
    "popfile = op.join(DIR,'SNPmat_popNames_%s.txt')\n",
    "\n",
    "pops.to_csv(popfile,header=False,index=False,sep=\"\\\\t\")\n",
    "print \"made popfile\"\n",
    "\n",
    "locfile = op.join(DIR,'SNPmat_locusNames_%s.txt')\n",
    "cols = pd.DataFrame(snpmat.columns)\n",
    "cols.to_csv(locfile,header=False,index=False,sep=\"\\\\t\")\n",
    "print \"done\"\n",
    "''' % ((i+2),str(i+2).zfill(3),str(i+2).zfill(3),str(i+2).zfill(3),str(i+2).zfill(3))\n",
    "    DIR = \"/home/lindb/wbp/outflank_null/make_infiles\"\n",
    "    if not op.exists(DIR):\n",
    "        os.makedirs(DIR)\n",
    "    filE = op.join(DIR,\"make_infiles_%s.py\") % (str(i+2).zfill(3))\n",
    "    with open(filE,'w') as o:\n",
    "        o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = \"/home/lindb/wbp/outflank_null/make_infiles\"\n",
    "files = [op.join(DIR,f) for f in ls(DIR) if '.py' in f]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir as ls\n",
    "from os import path as op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/outflank_null/'\n",
    "dirs = [op.join(DIR,d) for d in ls(DIR) if 'null' in d]\n",
    "rDIR = op.join(\"/home/lindb/wbp/outflank_null\",'r_files')\n",
    "for d in dirs:\n",
    "    num = op.basename(d).split(\"_\")[1]\n",
    "    if not num == '001':\n",
    "        text = '''\n",
    "library(OutFLANK)\n",
    "library(data.table)\n",
    "\n",
    "SNPmat = data.frame(fread('%s/SNPmat_noHEADERIDX_%s.txt',header=F,sep=\"\\\\t\"))\n",
    "\n",
    "locusNames = read.csv('%s/SNPmat_locusNames_%s.txt',header=F,sep=\"\\\\t\")\n",
    "\n",
    "popNames = read.csv('%s/SNPmat_popNames_%s.txt',header=F,sep=\"\\\\t\")\n",
    "\n",
    "FstDataFrame = MakeDiploidFSTMat(SNPmat,locusNames,popNames)\n",
    "\n",
    "out = OutFLANK(FstDataFrame = FstDataFrame,NumberOfSamples = 8)\n",
    "\n",
    "df = out$results\n",
    "\n",
    "outliers = df[which(df$OutlierFlag == 'TRUE'),]\n",
    "\n",
    "loci = outliers$LocusName\n",
    "\n",
    "write.table(df,'%s/OutFLANK_results_%s.txt',row.names=F,col.names=T,sep='\\\\t')\n",
    "\n",
    "write.table(loci,'%s/OutFLANK_snps_%s.txt',row.names=F,sep='\\\\t')\n",
    "\n",
    "print(\"DONE!\")\n",
    "''' % (d,num,\n",
    "       d,num,\n",
    "       d,num,\n",
    "       d,num,\n",
    "       d,num)\n",
    "        filE = op.join(rDIR,\"null_outflank_%s.R\") % num\n",
    "        if not op.exists(rDIR):\n",
    "            os.makedirs(rDIR)\n",
    "        with open(filE,'w') as o:\n",
    "            o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = \"/home/lindb/wbp/outflank_null/make_infiles\"\n",
    "files = [op.join(DIR,f) for f in ls(DIR) if '.py' in f]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = \"/home/lindb/wbp/outflank_null/make_infiles\"\n",
    "files = [op.join(DIR,f) for f in ls(DIR) if '.py' in f]\n",
    "count = 0\n",
    "scount = 0\n",
    "for f in files:\n",
    "    num = op.basename(f).split(\"_\")[2].split(\".\")[0]\n",
    "    if count == 0:\n",
    "        text = '''#!/usr/bin/bash\n",
    "#$ -N inf%s\n",
    "#$ -S /bin/bash\n",
    "#$ -V\n",
    "#$ -j y\n",
    "\n",
    "cd /home/lindb/wbp/outflank_null/make_infiles/\n",
    "python < make_infiles_%s.py\n",
    "R --no-save < /home/lindb/wbp/outflank_null/r_files/null_outflank_%s.R\n",
    "''' % (scount,num,num)\n",
    "    else:\n",
    "        newtext = '''\n",
    "cd /home/lindb/wbp/outflank_null/make_infiles/\n",
    "python < make_infiles_%s.py\n",
    "R --no-save < /home/lindb/wbp/outflank_null/r_files/null_outflank_%s.R\n",
    "''' % (num,num)\n",
    "        text = text + newtext\n",
    "    count += 1\n",
    "    if count == 10 or (count ==9 and scount == 9):\n",
    "        count = 0\n",
    "        \n",
    "        filE = op.join(DIR,\"run_them_all_%s.sh\" % scount)\n",
    "        with open(filE,'w') as o:\n",
    "            o.write(\"%s\" % text)\n",
    "        scount += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#using tmux on godel06 and godel97 to run the stupid null files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/outflank_null/'\n",
    "dirs = [op.join(DIR,d) for d in ls(DIR) if 'null' in d]\n",
    "for d in dirs:\n",
    "    files = [op.join(d,f) for f in ls(d)]\n",
    "    if len(files) > 0:\n",
    "        if len(files) == 6:\n",
    "            print op.basename(d),\"done\"\n",
    "        else:\n",
    "            print op.basename(d),len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# retrieve significant SNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#see how many sig snps I'm getting\n",
    "DIR = \"/home/lindb/wbp/outflank_null/\"\n",
    "dirs = [op.join(DIR,d) for d in ls(DIR) if 'null' in d]\n",
    "sigs = []\n",
    "for d in dirs:\n",
    "    files = [op.join(d,f) for f in ls(d) if 'snps' in f.split(\"_\")[:-1]]\n",
    "    if len(files) >0:\n",
    "        df = pd.read_csv(files[0])\n",
    "        sigs.append(len(df.index))\n",
    "    else:\n",
    "        print op.basename(d)\n",
    "plt.hist(sigs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(sigs), np.max(sigs), np.median(sigs),len(sigs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make pickle files for easier upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/outflank_null/make_pickles'\n",
    "if not op.exists(DIR):\n",
    "    os.makedirs(DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/outflank_null/'\n",
    "dirs = [op.join(DIR,d) for d in ls(DIR) if 'null' in d]\n",
    "for d in dirs:\n",
    "    num = op.basename(d).split(\"_\")[1]\n",
    "    filE = op.join(d,'SNPmat_HEADERIDX_%s.txt' % num)\n",
    "    if op.exists(filE):\n",
    "        filE2 = op.join(d,'SNPmat_HEADERIDX_%s.pkl' % num)\n",
    "        text = '''\n",
    "from __future__ import division\n",
    "import os\n",
    "from collections import OrderedDict,Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vcf\n",
    "from operator import itemgetter\n",
    "import random\n",
    "import math\n",
    "from os import listdir as ls\n",
    "from os import path as op\n",
    "import pickle\n",
    "\n",
    "df = pd.read_csv('%s',header=0,index_col=0,sep=\"\\\\t\")       ###########\n",
    "with open('%s','wb') as o:                                  ###########\n",
    "    pickle.dump(df,o,pickle.HIGHEST_PROTOCOL)\n",
    "''' % (filE,\n",
    "       filE2\n",
    "      )\n",
    "        filE3 = op.join(DIR,'make_pickles/%s.py' % num)\n",
    "        with open(filE3,'w') as o:\n",
    "            o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/outflank_null/make_pickles/'\n",
    "files = [op.join(DIR,f) for f in ls(DIR) if 'py' in f]\n",
    "#len(files)\n",
    "for f in files:\n",
    "    num = op.basename(f).split(\".\")[0]\n",
    "    text = '''#!/usr/bin/bash\n",
    "#$ -N inf%s\n",
    "#$ -S /bin/bash\n",
    "#$ -V\n",
    "#$ -j y\n",
    "\n",
    "cd /home/lindb/wbp/outflank_null/make_pickles/\n",
    "python %s\n",
    "''' % (num,f)\n",
    "    filE = op.join(DIR,\"run_%s.sh\" % num)\n",
    "    with open(filE,'w') as o:\n",
    "        o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/outflank_null/make_pickles/'\n",
    "files = [op.join(DIR,f) for f in ls(DIR) if 'sh' in f]\n",
    "for f in files:\n",
    "#    !qsub $f\n",
    "# i executed this in terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#see how many pickle files\n",
    "DIR = '/home/lindb/wbp/outflank_null/'\n",
    "dirs = [op.join(DIR,d) for d in ls(DIR) if 'null' in d]\n",
    "files = []\n",
    "for d in dirs:\n",
    "    num = op.basename(d).split(\"_\")[1]\n",
    "    filE = op.join(d,'SNPmat_HEADERIDX_%s.pkl' % num) \n",
    "    if op.exists(filE):\n",
    "        files.append(filE)\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# covariances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make glob dictionary a pickle for quicker loading\n",
    "filE  = '/home/lindb/wbp/OutFLANK/global_mafs.txt'\n",
    "df = pd.read_csv(filE,header=0,index_col=0,sep=\"\\t\")\n",
    "filE2 = '/home/lindb/wbp/OutFLANK/global_mafs.pkl'\n",
    "#with open(filE2,'wb') as o:\n",
    "    pickle.dump(df,o,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get pop-level maf, calc focal dij, create 1000 sets of null loci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = \"/home/lindb/wbp/outflank_null/\"\n",
    "dirs = [op.join(DIR,d) for d in ls(DIR) if 'null' in d]\n",
    "for d in dirs:\n",
    "    num = op.basename(d).split(\"_\")[1]\n",
    "    text = '''\n",
    "from __future__ import division\n",
    "import os\n",
    "from collections import OrderedDict,Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vcf\n",
    "from operator import itemgetter\n",
    "import random\n",
    "import math\n",
    "from os import listdir as ls\n",
    "from os import path as op\n",
    "import skbio\n",
    "import shutil\n",
    "import pickle\n",
    "\n",
    "\n",
    "#get global minor allele frequencies\n",
    "print \"getting global maf\"\n",
    "glob = pickle.load(open(\"/home/lindb/wbp/OutFLANK/global_mafs.pkl\",\"rb\"))\n",
    "\n",
    "############################### Get pop-level MAFs #################################################\n",
    "#get 012 data with population assignments\n",
    "print \"uploading 012\"\n",
    "filE = \"%s\"                                                          ###############################\n",
    "data = pickle.load(open(filE,\"rb\"))\n",
    "\n",
    "print \"merging data frames\"\n",
    "m = pd.read_csv(\"%s\", header=None,index_col=None,sep=\"\\\\t\")          ###############################\n",
    "data[\"pop\"] = m[0].tolist()\n",
    "cols = [\"pop\"] + [col for col in data.columns if \"NODE\" in col]\n",
    "data = data[cols]\n",
    "\n",
    "#get a list of samps for each population\n",
    "print \"getting samp lists per pop - these samps have been randomly assigned\"\n",
    "pops = np.unique(data[\"pop\"]).tolist()\n",
    "popDict = OrderedDict()\n",
    "for samp in data.index:\n",
    "    pop = data.loc[samp,\"pop\"]\n",
    "    if not pop in popDict.keys():\n",
    "        popDict[pop] = []\n",
    "    popDict[pop].append(samp)\n",
    "\n",
    "#get dataframes for each pop based on samples assigned to that pop\n",
    "print \"getting pop-level dfs\"\n",
    "popDictImp = OrderedDict()\n",
    "for pop in sorted(pops):\n",
    "    popDictImp[pop]  = data[data.index.isin(popDict[pop])]\n",
    "\n",
    "#get major and minor allele counts per pop\n",
    "print \"determining mjr mnr counts\"\n",
    "filE = \"%s\"                                                          ###############################\n",
    "if not op.exists(filE):\n",
    "    locCount = 0\n",
    "    Dict = OrderedDict()\n",
    "    for locus in data.columns[data.columns != 'pop']:\n",
    "        Dict[locus] = OrderedDict()\n",
    "        popCount = 0\n",
    "        for pop in sorted(pops):\n",
    "\n",
    "            zero = popDictImp[pop][locus].tolist().count(0) #count the first homozygotes\n",
    "            one = popDictImp[pop][locus].tolist().count(1) #count the heterozygotes\n",
    "            two = popDictImp[pop][locus].tolist().count(2) #count the second homozygotes        \n",
    "\n",
    "            #012 counts global minor allele\n",
    "            A1 = 2*zero + one\n",
    "            A2 = 2*two + one\n",
    "\n",
    "            #no need to distinguish A1 > A2, since MAF within pop refers to global minor allele\n",
    "\n",
    "            if len(Dict[locus].keys()) == 0:\n",
    "                Dict[locus][\"A1\"] = OrderedDict()\n",
    "                Dict[locus][\"A2\"] = OrderedDict()\n",
    "            Dict[locus][\"A1\"][pop] = A1\n",
    "            Dict[locus][\"A2\"][pop] = A2\n",
    "            #break\n",
    "        locCount += 1\n",
    "        if locCount %% 1000 == 0:\n",
    "            print locCount\n",
    "\n",
    "    print \"writing mjr mnr count file\"\n",
    "    with open(filE, \"w\") as o:\n",
    "        line = \"\\\\t\".join([str(pop) for pop in Dict[Dict.keys()[0]][Dict[Dict.keys()[0]].keys()[0]].keys()]) + str(\"\\\\n\")\n",
    "        o.write(\"%%s\" %% line)\n",
    "        for locus in Dict.keys():\n",
    "            for allele in Dict[locus].keys():\n",
    "                line = str(locus) + \"\\\\t\" + \"\\\\t\".join([str(x) for x in Dict[locus][allele].values()]) + str(\"\\\\n\")\n",
    "                #print locus,allele,line\n",
    "                o.write(\"%%s\" %% line)\n",
    "\n",
    "else:\n",
    "    print \"mjr mnr count file already exists, skipping calculation\"\n",
    "\n",
    "#get allele counts by pop - first locus = counts of 0 allele, second = counts of 2 allele\n",
    "    #012 counts global minor allele\n",
    "counts = pd.read_csv(filE,header=0,index_col=0,sep=\"\\\\t\")\n",
    "\n",
    "loci = np.unique(counts.index).tolist()\n",
    "print \"getting pop-level maf\"\n",
    "filE = \"%s\"                                                           ###############################\n",
    "if not op.exists(filE):\n",
    "    loccount = 0\n",
    "    mafDict = OrderedDict()\n",
    "    for locus in loci:\n",
    "        mafDict[locus] = OrderedDict()\n",
    "        DATA = pd.DataFrame(counts.loc[locus,:])\n",
    "        DATA.index = [\"major\",\"minor\"]\n",
    "        for pop in DATA.columns:\n",
    "            MAF = DATA.loc[\"minor\",pop]/sum(DATA[pop]) #get allele freq corresponding to global minor allele\n",
    "            mafDict[locus][pop] = MAF\n",
    "        loccount += 1\n",
    "        if loccount %% 1000 == 0:\n",
    "            print loccount\n",
    "\n",
    "    print \"writing pop-level maf\"\n",
    "    with open(filE,\"w\") as o:\n",
    "        text = \"\\\\t\".join([x for x in mafDict[mafDict.keys()[0]].keys()]) + \"\\\\n\"\n",
    "        o.write(\"%%s\" %% text)\n",
    "        print text\n",
    "        count = 0\n",
    "        for locus in mafDict.keys():\n",
    "            text = locus + \"\\\\t\" + \"\\\\t\".join([str(x) for x in mafDict[locus].values()]) + \"\\\\n\"\n",
    "            o.write(\"%%s\" %% text)\n",
    "            count += 1\n",
    "else:\n",
    "    print \"pop-level maf already created, skipping calculation\"\n",
    "\n",
    "print \"reading pop-level maf\"\n",
    "impMAF = pd.read_csv(filE,header=0,index_col=0,sep=\"\\\\t\")\n",
    "##############################################################################################\n",
    "\n",
    "#get outlier snps\n",
    "print \"getting outlier snps\"\n",
    "df = pd.read_csv(\"%s\",header=0,sep=\"\\\\t\")                             ###############################\n",
    "outliersnps = df[\"x\"].tolist()\n",
    "\n",
    "#do pairwise to get Dij for outliers\n",
    "print \"getting pop counts\"\n",
    "popcounts = OrderedDict()\n",
    "for pop in pops:\n",
    "    popcounts[pop] = len(popDict[pop])\n",
    "\n",
    "print \"getting dij for outliers\"\n",
    "dijDict = OrderedDict() \n",
    "icount = 0\n",
    "for i,locusi in enumerate(outliersnps):\n",
    "    dijDict[locusi] = OrderedDict()\n",
    "    qi = glob[locusi] #global maf\n",
    "    \n",
    "    for j,locusj in enumerate(outliersnps):\n",
    "        if i > j: #i=row, j=col : lower triangle \n",
    "            qj = glob[locusj] #global maf\n",
    "            \n",
    "            sums = 0\n",
    "            for pop in impMAF.columns:\n",
    "                qik = impMAF.loc[locusi,pop] #get pop maf\n",
    "                qjk = impMAF.loc[locusj,pop] #get pop maf\n",
    "                nk = popcounts[pop]\n",
    "                \n",
    "                sums += (nk/244)*((qik*qjk)-(qi*qj))\n",
    "\n",
    "            dijDict[locusi][locusj] = sums\n",
    "        else:\n",
    "            dijDict[locusi][locusj] = np.nan\n",
    "    icount += 1\n",
    "    if icount %% 10 == 0:\n",
    "        print icount\n",
    "#write out the file\n",
    "print \"writing dij for outliers\"\n",
    "rowcount = 0\n",
    "filE = \"%s\"                                                          ###############################\n",
    "with open(filE,\"w\") as o:\n",
    "    key0 = dijDict.keys()[0]\n",
    "    line = \"\\\\t\".join(dijDict[key0].keys()) + str(\"\\\\n\")\n",
    "    o.write(\"%%s\" %% line)\n",
    "    for locusi in dijDict.keys():\n",
    "        line = str(locusi)+\"\\\\t\"+\"\\\\t\".join([str(x) for x in dijDict[locusi].values()]) + str(\"\\\\n\")\n",
    "        o.write(\"%%s\" %% line)\n",
    "dvals = pd.read_csv(filE,header=0,index_col=0,sep=\"\\\\t\")\n",
    "\n",
    "#get global expected heterozygosity and bins\n",
    "print \"getting global het bins\"\n",
    "filE = \"/home/lindb/wbp/OutFLANK/Hexp_by_snp_withbins.txt\"\n",
    "H = pd.read_csv(filE, header=0,index_col=0,sep=\"\\\\t\")\n",
    "H.index = [snp for snp in H[\"locus\"].tolist()]\n",
    "\n",
    "#get a dataframe with the outlier loci and their bins\n",
    "print \"getting bins for outlier data\"\n",
    "outlierdata = pd.DataFrame(H[H[\"locus\"].isin(outliersnps)])\n",
    "outlierdata.index = [snp for snp in outlierdata[\"locus\"].tolist()]\n",
    "\n",
    "print \"determining non-outliers for null draws\"\n",
    "nonsigs = set(H.index.tolist()) - set(outlierdata.index.tolist())\n",
    "nonsigs = [x for x in nonsigs]\n",
    "nonsigdata = pd.DataFrame(H[H[\"locus\"].isin(nonsigs)])\n",
    "\n",
    "#how many random snps from each bin?\n",
    "print \"creating binCounter for outlier loci\"\n",
    "binCounter = Counter()\n",
    "for row in outlierdata.index:\n",
    "    binCounter[outlierdata.loc[row,\"bin\"]] += 1\n",
    "\n",
    "folder = \"%s\"                                                       ###############################\n",
    "if not op.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "#make 1000 dataframes of random snps of equal size\n",
    "print \"making 1000 dfs of random snps\"\n",
    "for i in range(1000):\n",
    "    snps = []        \n",
    "    for binn in binCounter.keys():\n",
    "        data = nonsigdata[nonsigdata[\"bin\"] == binn]\n",
    "\n",
    "        [snps.append(snp) for snp in random.sample(data.index,binCounter[binn])]\n",
    "\n",
    "    filE = op.join(folder,\"outflank_null_%%s_randsnps.txt\" %% (str(i).zfill(4)))\n",
    "    df = pd.DataFrame(snps)\n",
    "    df.to_csv(filE,header=False,index=False,sep=\"\\\\t\")\n",
    "''' % (op.join(d,'SNPmat_HEADERIDX_%s.pkl' % num),\n",
    "       op.join(d,'SNPmat_popNames_%s.txt' % num),\n",
    "       op.join(d,'UnbinnedImputedSNPSFILE_%s.txt' % num),\n",
    "       op.join(d,'imputed_MAF_%s.txt' % num),\n",
    "       op.join(d,'OutFLANK_snps_%s.txt' % num),\n",
    "       op.join(d,'imputed_dvals_%s.txt' % num),\n",
    "       op.join(d,'covariances/randmatrices/randsnps')\n",
    "       )\n",
    "    filE = op.join(d,'get_dij_%s.py' % num)\n",
    "    with open(filE,'w') as o:\n",
    "        o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = \"/home/lindb/wbp/outflank_null/\"\n",
    "dirs = [op.join(DIR,d) for d in ls(DIR) if 'null' in d]\n",
    "files = []\n",
    "for d in dirs:\n",
    "    num = op.basename(d).split(\"_\")[1]\n",
    "    f = op.join(d,'get_dij_%s.py' % num)\n",
    "    assert op.exists(f)\n",
    "    files.append(f)\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/outflank_null/make_null_sets'\n",
    "if not op.exists(DIR):\n",
    "    os.makedirs(DIR)\n",
    "for f in sorted(files):\n",
    "    num = op.dirname(f).split(\"/\")[-1].split(\"_\")[1]\n",
    "    text = '''#!/usr/bin/bash\n",
    "#$ -N inf%s\n",
    "#$ -S /bin/bash\n",
    "#$ -V\n",
    "#$ -j y\n",
    "\n",
    "cd %s\n",
    "python %s\n",
    "''' % (num,\n",
    "       op.dirname(f),\n",
    "       f\n",
    "      )\n",
    "    filE = op.join(DIR,'make_null_%s.sh' % num)\n",
    "    with open(filE,'w') as o:\n",
    "        o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/outflank_null/make_null_sets'\n",
    "files = [op.join(DIR,f) for f in ls(DIR)]\n",
    "for f in files:\n",
    "    #!qsub $f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check on progress\n",
    "DIR = '/home/lindb/wbp/outflank_null/'\n",
    "dirs = [op.join(DIR,d) for d in ls(DIR) if 'null' in d and 'make' not in d]\n",
    "count = 0\n",
    "for d in sorted(dirs):\n",
    "    num = op.basename(d).split(\"_\")[1]\n",
    "    DIR = op.join(d,'covariances/randmatrices/randsnps')\n",
    "    if op.exists(DIR):\n",
    "        #files = [f for f in ls(DIR)]\n",
    "        #print num, len(files)\n",
    "        count += 1\n",
    "print '\\n',count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get dij for random snp sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/outflank_null/'\n",
    "dirs = [op.join(DIR,d) for d in ls(DIR) if 'null' in d and 'make' not in d]\n",
    "for d in dirs:\n",
    "    num = op.basename(d).split(\"_\")[1]\n",
    "    DIR = op.join(d, 'covariances/randmatrices/randsnps/')\n",
    "    files = [op.join(DIR,f) for f in ls(DIR)]\n",
    "    assert len(files) == 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/outflank_null/'\n",
    "dirs = sorted([op.join(DIR,d) for d in ls(DIR) if 'null' in d and 'make' not in d])\n",
    "for d in dirs:\n",
    "    num = op.basename(d).split(\"_\")[1]\n",
    "    text = '''from __future__ import division\n",
    "import os\n",
    "from collections import OrderedDict,Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vcf\n",
    "from operator import itemgetter\n",
    "import random\n",
    "import math\n",
    "from os import listdir as ls\n",
    "from os import path as op\n",
    "import skbio\n",
    "import shutil\n",
    "import pickle\n",
    "\n",
    "#get global minor allele frequencies\n",
    "print \"getting global maf\"\n",
    "glob = pickle.load(open(\"/home/lindb/wbp/OutFLANK/global_mafs.pkl\",\"rb\"))\n",
    "\n",
    "#get 012 data with population assignments\n",
    "print \"uploading 012\"\n",
    "filE = \"%s\"                                                          ###############################\n",
    "data = pickle.load(open(filE,\"rb\"))\n",
    "\n",
    "print \"merging data frames\"\n",
    "m = pd.read_csv(\"%s\", header=None,index_col=None,sep=\"\\\\t\")          ###############################\n",
    "data[\"pop\"] = m[0].tolist()\n",
    "cols = [\"pop\"] + [col for col in data.columns if \"NODE\" in col]\n",
    "data = data[cols]\n",
    "\n",
    "#get a list of samps for each population\n",
    "print \"getting samp lists per pop - these samps have been randomly assigned\"\n",
    "pops = np.unique(data[\"pop\"]).tolist()\n",
    "popDict = OrderedDict()\n",
    "for samp in data.index:\n",
    "    pop = data.loc[samp,\"pop\"]\n",
    "    if not pop in popDict.keys():\n",
    "        popDict[pop] = []\n",
    "    popDict[pop].append(samp)\n",
    "\n",
    "print \"getting pop counts\"\n",
    "popcounts = OrderedDict()\n",
    "for pop in pops:\n",
    "    popcounts[pop] = len(popDict[pop])\n",
    "    \n",
    "popMAF = pd.read_csv(\"%s\",header=0,index_col=0,sep=\"\\\\t\")            ###############################\n",
    "\n",
    "print \"calculating dij for 1000 sets of null snps\"\n",
    "DIR = \"%s\"                                                           ###############################\n",
    "files = [op.join(DIR,f) for f in ls(DIR)]\n",
    "assert len(files) == 1000\n",
    "COUNTS = 0\n",
    "for f in sorted(files):\n",
    "    num = op.basename(f).split(\"_\")[2]\n",
    "    df = pd.read_csv(f, header=None,sep=\"\\\\t\")\n",
    "    randomsnps = df[0].tolist()\n",
    "\n",
    "    dijDict = OrderedDict() \n",
    "    for i,locusi in enumerate(randomsnps):\n",
    "        dijDict[locusi] = OrderedDict()\n",
    "        qi = glob[locusi] #global maf\n",
    "\n",
    "        for j,locusj in enumerate(randomsnps):\n",
    "            if i > j: #i=row, j=col : lower triangle \n",
    "                qj = glob[locusj] #global maf\n",
    "\n",
    "                sums = 0\n",
    "                for pop in popMAF.columns:\n",
    "                    qik = popMAF.loc[locusi,pop] #get pop maf\n",
    "                    qjk = popMAF.loc[locusj,pop] #get pop maf\n",
    "                    nk = popcounts[pop]\n",
    "\n",
    "                    sums += (nk/244)*((qik*qjk)-(qi*qj))\n",
    "\n",
    "                dijDict[locusi][locusj] = sums\n",
    "            else:\n",
    "                dijDict[locusi][locusj] = np.nan\n",
    "    DIR = \"%s\"                                                       ###############################\n",
    "    if not op.exists(DIR):\n",
    "        os.makedirs(DIR)\n",
    "\n",
    "    filE = op.join(DIR,\"outflank_null_%%s_null_DVALS.txt\" %% num)   \n",
    "    with open(filE,'w') as o:\n",
    "        key0 = dijDict.keys()[0]\n",
    "        line = '\\\\t'.join(dijDict[key0].keys()) + str('\\\\n')\n",
    "        o.write(\"%%s\" %% line)\n",
    "        for locusi in dijDict.keys():\n",
    "            line = str(locusi)+'\\\\t'+'\\\\t'.join([str(x) for x in dijDict[locusi].values()]) + str('\\\\n')\n",
    "            o.write(\"%%s\" %% line)\n",
    "    COUNTS += 1\n",
    "    if COUNTS %% 10 == 0:\n",
    "        print num, COUNTS\n",
    "\n",
    "\n",
    "''' % (op.join(d,'SNPmat_HEADERIDX_%s.pkl' % num),\n",
    "       op.join(d,'SNPmat_popNames_%s.txt' % num),\n",
    "       op.join(d,'imputed_MAF_%s.txt' % num),\n",
    "       op.join(d,'covariances/randmatrices/randsnps/'),\n",
    "       op.join(d,'covariances/randmatrices/randdij/')\n",
    "      )\n",
    "    DIR = '/home/lindb/wbp/outflank_null/get_rand_dij'\n",
    "    if not op.exists(DIR):\n",
    "        os.makedirs(DIR)\n",
    "    filE = op.join(DIR,'get_rand_dij_%s.py' % num)\n",
    "    with open(filE,'w') as o:\n",
    "        o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/outflank_null/get_rand_dij'\n",
    "files = [op.join(DIR,f) for f in ls(DIR)]\n",
    "for f in files:\n",
    "    num = op.basename(f).split(\".\")[0].split(\"_\")[-1]\n",
    "    text = '''#!/usr/bin/bash\n",
    "#$ -N inf%s\n",
    "#$ -S /bin/bash\n",
    "#$ -V\n",
    "#$ -j y\n",
    "\n",
    "cd %s\n",
    "python %s\n",
    "''' % (num,\n",
    "       op.dirname(f),\n",
    "       f\n",
    "      )\n",
    "    filE = op.join(DIR,'go_get_rand_dij_%s.sh' % num)\n",
    "    with open(filE,'w') as o:\n",
    "        o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/outflank_null/get_rand_dij'\n",
    "files = [op.join(DIR,f) for f in ls(DIR) if 'sh' in f]\n",
    "for f in sorted(files):\n",
    "    !qsub $f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check on progress\n",
    "DIR = '/home/lindb/wbp/outflank_null/'\n",
    "dirs = [op.join(DIR,d) for d in ls(DIR) if 'null' in d and 'make' not in d]\n",
    "count = 0\n",
    "COUNT = 0\n",
    "for d in sorted(dirs):\n",
    "    num = op.basename(d).split(\"_\")[1]\n",
    "    DIR = op.join(d,'covariances/randmatrices/randdij/')\n",
    "    if op.exists(DIR):\n",
    "        files = [f for f in ls(DIR)]\n",
    "        #print num, len(files)\n",
    "        if len(files) == 1000:\n",
    "            count += 1\n",
    "            #print op.basename(d)\n",
    "            #print num\n",
    "        if len(files) < 1000:\n",
    "            print op.basename(d), len(files)\n",
    "        COUNT += 1\n",
    "print '\\n',count\n",
    "print '\\n',COUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# is there higher covariance than random sets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for each null run of outflank\n",
    "    #get the distribution of 1000 medians\n",
    "    #get the median focal dij ('emp')\n",
    "    #what is the percentile of 'emp' in the distriubtion of 1000 medians?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/outflank_null/'\n",
    "dirs = sorted([op.join(DIR,d) for d in ls(DIR) if 'null' in d and 'make' not in d])\n",
    "for d in dirs:\n",
    "    num = op.basename(d).split(\"_\")[1]\n",
    "    text = '''from __future__ import division\n",
    "import os\n",
    "from collections import OrderedDict,Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vcf\n",
    "from operator import itemgetter\n",
    "import random\n",
    "import math\n",
    "from os import listdir as ls\n",
    "from os import path as op\n",
    "import skbio\n",
    "import shutil\n",
    "import pickle\n",
    "\n",
    "print \"getting empirical dij value\"\n",
    "filE = \"%s\"                                                       ###############################\n",
    "print filE\n",
    "DF = pd.read_csv(filE,header=0,index_col=0,sep=\"\\\\t\")\n",
    "empdijs = []\n",
    "for i,locusi in enumerate(DF.index):\n",
    "    for j,locusj in enumerate(DF.columns):\n",
    "        if i > j:\n",
    "            empdijs.append(abs(float(DF.loc[locusi,locusj])))\n",
    "empmed = np.median(empdijs)\n",
    "\n",
    "DIR = \"%s\"                                                        ###############################\n",
    "files = [op.join(DIR,f) for f in ls(DIR)]\n",
    "assert len(files) == 1000\n",
    "randmeds = []\n",
    "for f in sorted(files):\n",
    "    df = pd.read_csv(f,header=0,index_col=0,sep=\"\\\\t\")\n",
    "    vals = []\n",
    "    for i,row in enumerate(df.index):\n",
    "        for j,col in enumerate(df.columns):\n",
    "            if i > j:\n",
    "                vals.append(abs(float(df.loc[row,col])))\n",
    "    med = np.median(vals)\n",
    "    randmeds.append(med)\n",
    "\n",
    "for i,val in enumerate(sorted(randmeds)):\n",
    "    if empmed > val:\n",
    "        perc = (i+1)/len(randmeds)\n",
    "        \n",
    "dataf = pd.DataFrame()\n",
    "dataf['randmeds'] = sorted(randmeds)\n",
    "dataf.loc[0,'empmed'] = empmed\n",
    "dataf.loc[0,'percentile'] = perc\n",
    "dataf.loc[0,'greater_than_max'] = empmed / max(randmeds)\n",
    "dataf.loc[0,'greater_than_n5th'] = empmed / sorted(randmeds)[int(round(len(randmeds)*0.95))]\n",
    "print len(DF.index)\n",
    "#dataf.loc[0,'len_outliers'] = len(DF.index)\n",
    "\n",
    "filE = \"%s\"                                                       ###############################\n",
    "dataf.to_csv(filE,header=True,index=False,sep=\"\\\\t\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "''' % (op.join(d,'imputed_dvals_%s.txt' % num),\n",
    "       op.join(d,'covariances/randmatrices/randdij'),\n",
    "       op.join(d,'results_%s.txt' % num)\n",
    "      )\n",
    "    DIR = '/home/lindb/wbp/outflank_null/get_results'\n",
    "    if not op.exists(DIR):\n",
    "        os.makedirs(DIR)\n",
    "    filE = op.join(DIR,'get_results_%s.py' % num)\n",
    "    with open(filE,'w') as o:\n",
    "        o.write(\"%s\" % text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/outflank_null/get_results'\n",
    "files = [op.join(DIR,f) for f in ls(DIR) if 'sh' not in f]\n",
    "assert len(files) == 100\n",
    "for f in sorted(files):\n",
    "    num = op.basename(f).split(\".\")[0].split(\"_\")[2]\n",
    "    text = '''#!/usr/bin/bash\n",
    "#$ -N inf%s\n",
    "#$ -S /bin/bash\n",
    "#$ -V\n",
    "#$ -j y\n",
    "\n",
    "cd %s\n",
    "python %s\n",
    "''' % (num,\n",
    "       DIR,\n",
    "       f\n",
    "      )\n",
    "    filE = op.join(DIR,'go_get_results_%s.sh' % num)\n",
    "    with open(filE,'w') as o:\n",
    "        o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check on progress\n",
    "DIR = '/home/lindb/wbp/outflank_null/'\n",
    "dirs = [op.join(DIR,d) for d in ls(DIR) if 'null' in d and 'make' not in d]\n",
    "assert len(dirs) == 100\n",
    "count = 0\n",
    "for d in dirs:\n",
    "    num = op.basename(d).split(\"_\")[1]\n",
    "    filE = op.join(d,'results_%s.txt' % num)\n",
    "    if op.exists(filE):\n",
    "        count += 1\n",
    "        os.remove(filE)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get the results\n",
    "DIR = '/home/lindb/wbp/outflank_null/'\n",
    "dirs = [op.join(DIR,d) for d in ls(DIR) if 'null' in d and 'make' not in d]\n",
    "assert len(dirs) == 100\n",
    "percs = []\n",
    "for d in dirs:\n",
    "    num = op.basename(d).split(\"_\")[1]\n",
    "    filE = op.join(d,'results_%s.txt' % num)\n",
    "    if op.exists(filE):\n",
    "        df = pd.read_csv(filE,header=0,sep='\\t')\n",
    "        percs.append(df.loc[0,'percentile'])\n",
    "plt.hist(percs), len(percs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get the results\n",
    "DIR = '/home/lindb/wbp/outflank_null/'\n",
    "dirs = [op.join(DIR,d) for d in ls(DIR) if 'null' in d and 'make' not in d]\n",
    "assert len(dirs) == 100\n",
    "gts = []\n",
    "for d in dirs:\n",
    "    num = op.basename(d).split(\"_\")[1]\n",
    "    filE = op.join(d,'results_%s.txt' % num)\n",
    "    if op.exists(filE):\n",
    "        df = pd.read_csv(filE,header=0,sep='\\t')\n",
    "        gts.append(df.loc[0,'greater_than_max'])\n",
    "    else:\n",
    "        print filE\n",
    "plt.hist(gts), len(gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.median(gts),np.mean(gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.std(gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(10.6-np.mean(gts))/np.std(gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "10.6/np.median(gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "10.6/max(gts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Using a different method to randomize individuals in populations (just in case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get necessary dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glob = pickle.load(open(\"/home/lindb/wbp/OutFLANK/global_mafs.pkl\",\"rb\"))\n",
    "\n",
    "#z12 file created in 06_pca.ipyn\n",
    "filE = '/home/lindb/wbp/OutFLANK/imputed_z12_maf_swp_trans_z12.pkl'\n",
    "imp012 = pickle.load(open(filE,'rb'))\n",
    "\n",
    "#get pop assignment for each samp\n",
    "print \"making stp df\"\n",
    "filE = '/home/lindb/wbp/sampsTOpop.txt'\n",
    "stp = pd.read_csv(filE,header=0,index_col='sampID',sep=\"\\\\t\")\n",
    "stp.sort_index(inplace=True)\n",
    "\n",
    "#assign samps to pop by random number\n",
    "samps = stp.index.tolist()\n",
    "stpnew = pd.DataFrame(stp)\n",
    "stpnew['runif'] = \"\"\n",
    "for row in stp.index:\n",
    "    stpnew.loc[row,'runif'] = random.random()\n",
    "stpnew.sort_values(by=['runif'],inplace = True)\n",
    "stpnew.index = [samp for samp in samps]\n",
    "stpnew.sort_index(inplace=True)\n",
    "stpnew.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged = pd.merge(imp012,stpnew,left_index=True,right_index=True) \n",
    "cols = ['pop'] + [col for col in merged.columns if 'NODE' in col] \n",
    "merged = merged[cols]\n",
    "merged.sort_index(inplace=True)\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = [col for col in merged.columns if 'NODE' in col] \n",
    "snpmat = merged[cols]\n",
    "snpmat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pops = pd.DataFrame(merged['pop'].tolist())\n",
    "pops.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/outflank_null/andrews'\n",
    "if not op.exists(DIR):\n",
    "    os.makedirs(DIR)\n",
    "\n",
    "print 'making 1st snpmat' \n",
    "filE = op.join(DIR,'SNPmat_HEADERIDX.txt')\n",
    "snpmat.to_csv(filE,header=True,index=True,sep=\"\\t\")\n",
    "\n",
    "print 'making 2nd snpmat' \n",
    "filE2 = op.join(DIR,'SNPmat_noHEADERIDX.txt')\n",
    "snpmat.to_csv(filE2,header=None,index=False,sep=\"\\t\")\n",
    "\n",
    "print \"making pickle\"\n",
    "pfile = op.join(DIR,'SNPmat_HEADERIDX.pkl')\n",
    "with open(pfile,'wb') as o:\n",
    "    pickle.dump(snpmat,o,pickle.HIGHEST_PROTOCOL)    \n",
    "\n",
    "print 'making popfile' \n",
    "popfile = op.join(DIR,'SNPmat_popNames.txt')\n",
    "pops.to_csv(popfile,header=False,index=False,sep=\"\\t\")\n",
    "\n",
    "print 'making locfile'\n",
    "locfile = op.join(DIR,'SNPmat_locusNames.txt')\n",
    "cols = pd.DataFrame(snpmat.columns) \n",
    "cols.to_csv(locfile,header=False,index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = '''\n",
    "library(OutFLANK)\n",
    "\n",
    "library(data.table)\n",
    "\n",
    "SNPmat = data.frame(fread('/home/lindb/wbp/outflank_null/andrews/SNPmat_noHEADERIDX.txt',header=F,sep=\"\\\\t\"))\n",
    "\n",
    "locusNames = read.csv('/home/lindb/wbp/outflank_null/andrews/SNPmat_locusNames.txt',header=F,sep=\"\\\\t\")\n",
    "\n",
    "popNames = read.csv('/home/lindb/wbp/outflank_null/andrews/SNPmat_popNames.txt',header=F,sep=\"\\\\t\")\n",
    "\n",
    "FstDataFrame = MakeDiploidFSTMat(SNPmat,locusNames,popNames)\n",
    "\n",
    "out = OutFLANK(FstDataFrame = FstDataFrame,NumberOfSamples = 8)\n",
    "\n",
    "df = out$results\n",
    "\n",
    "outliers = df[which(df$OutlierFlag == 'TRUE'),]\n",
    "\n",
    "loci = outliers$LocusName\n",
    "\n",
    "write.table(df,'/home/lindb/wbp/outflank_null/andrews/OutFLANK_results.txt',row.names=F,col.names=T,sep='\\\\t')\n",
    "\n",
    "write.table(loci,'/home/lindb/wbp/outflank_null/andrews/OutFLANK_snps.txt',row.names=F,sep='\\\\t')\n",
    "\n",
    "print(\"DONE!\")\n",
    "'''\n",
    "\n",
    "filE = \"/home/lindb/wbp/outflank_null/andrews/do_the_outflank.R\" \n",
    "\n",
    "with open(filE,'w') as o:\n",
    "    o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = \"/home/lindb/wbp/outflank_null/andrews\"\n",
    "sigs = []\n",
    "for d in [DIR]:\n",
    "    files = [op.join(d,'OutFLANK_snps.txt')] \n",
    "    if len(files) >0:\n",
    "        df = pd.read_csv(files[0])\n",
    "        sigs.append(len(df.index)) \n",
    "    else:\n",
    "        print op.basename(d) \n",
    "sigs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# covariances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = \"/home/lindb/wbp/outflank_null/andrews\"\n",
    "#get global minor allele frequencies\n",
    "print \"getting global maf\"\n",
    "glob = pickle.load(open(\"/home/lindb/wbp/OutFLANK/global_mafs.pkl\",\"rb\"))\n",
    "\n",
    "#get 012 data with population assignments\n",
    "print \"uploading 012\"\n",
    "filE = op.join(d,'SNPmat_HEADERIDX.pkl') \n",
    "data = pickle.load(open(filE,\"rb\"))\n",
    "\n",
    "print \"merging data frames\"\n",
    "m = pd.read_csv(op.join(d,'SNPmat_popNames.txt'), header=None,index_col=None,sep=\"\\t\")     \n",
    "data[\"pop\"] = m[0].tolist()\n",
    "cols = [\"pop\"] + [col for col in data.columns if \"NODE\" in col]\n",
    "data = data[cols]\n",
    "\n",
    "#get a list of samps for each population\n",
    "print \"getting samp lists per pop - these samps have been randomly assigned\"\n",
    "pops = np.unique(data[\"pop\"]).tolist()\n",
    "popDict = OrderedDict()\n",
    "for samp in data.index:\n",
    "    pop = data.loc[samp,\"pop\"]\n",
    "    if not pop in popDict.keys():\n",
    "        popDict[pop] = []\n",
    "    popDict[pop].append(samp)\n",
    "\n",
    "#get dataframes for each pop based on samples assigned to that pop\n",
    "print \"getting pop-level dfs\"\n",
    "popDictImp = OrderedDict()\n",
    "for pop in sorted(pops):\n",
    "    popDictImp[pop]  = data[data.index.isin(popDict[pop])]\n",
    "\n",
    "#get major and minor allele counts per pop\n",
    "print \"determining mjr mnr counts\"\n",
    "filE = op.join(d,'UnbinnedImputedSNPSFILE.txt')\n",
    "if not op.exists(filE):\n",
    "    locCount = 0\n",
    "    Dict = OrderedDict()\n",
    "    for locus in data.columns[data.columns != 'pop']:\n",
    "        Dict[locus] = OrderedDict()\n",
    "        popCount = 0\n",
    "        for pop in sorted(pops):\n",
    "\n",
    "            zero = popDictImp[pop][locus].tolist().count(0) #count the first homozygotes\n",
    "            one = popDictImp[pop][locus].tolist().count(1) #count the heterozygotes\n",
    "            two = popDictImp[pop][locus].tolist().count(2) #count the second homozygotes        \n",
    "\n",
    "            #012 counts global minor allele\n",
    "            A1 = 2*zero + one\n",
    "            A2 = 2*two + one\n",
    "\n",
    "            #no need to distinguish A1 > A2, since MAF within pop refers to global minor allele\n",
    "\n",
    "            if len(Dict[locus].keys()) == 0:\n",
    "                Dict[locus][\"A1\"] = OrderedDict()\n",
    "                Dict[locus][\"A2\"] = OrderedDict()\n",
    "            Dict[locus][\"A1\"][pop] = A1\n",
    "            Dict[locus][\"A2\"][pop] = A2\n",
    "            #break\n",
    "        locCount += 1\n",
    "        if locCount % 1000 == 0:\n",
    "            print locCount\n",
    "\n",
    "    print \"writing mjr mnr count file\"\n",
    "    with open(filE, \"w\") as o:\n",
    "        line = \"\\t\".join([str(pop) for pop in Dict[Dict.keys()[0]][Dict[Dict.keys()[0]].keys()[0]].keys()]) + str(\"\\n\")\n",
    "        o.write(\"%s\" % line)\n",
    "        for locus in Dict.keys():\n",
    "            for allele in Dict[locus].keys():\n",
    "                line = str(locus) + \"\\t\" + \"\\t\".join([str(x) for x in Dict[locus][allele].values()]) + str(\"\\n\")\n",
    "                #print locus,allele,line\n",
    "                o.write(\"%s\" % line)\n",
    "else:\n",
    "    print \"mjr mnr count file already exists, skipping calculation\"\n",
    "\n",
    "#get allele counts by pop - first locus = counts of 0 allele, second = counts of 2 allele\n",
    "    #012 counts global minor allele\n",
    "counts = pd.read_csv(filE,header=0,index_col=0,sep=\"\\t\")\n",
    "\n",
    "loci = np.unique(counts.index).tolist()\n",
    "print \"getting pop-level maf\"\n",
    "filE = op.join(d,'imputed_MAF.txt')\n",
    "if not op.exists(filE):\n",
    "    loccount = 0\n",
    "    mafDict = OrderedDict()\n",
    "    for locus in loci:\n",
    "        mafDict[locus] = OrderedDict()\n",
    "        DATA = pd.DataFrame(counts.loc[locus,:])\n",
    "        DATA.index = [\"major\",\"minor\"]\n",
    "        for pop in DATA.columns:\n",
    "            MAF = DATA.loc[\"minor\",pop]/sum(DATA[pop]) #get allele freq corresponding to global minor allele\n",
    "            mafDict[locus][pop] = MAF\n",
    "        loccount += 1\n",
    "        if loccount % 1000 == 0:\n",
    "            print loccount\n",
    "\n",
    "    print \"writing pop-level maf\"\n",
    "    with open(filE,\"w\") as o:\n",
    "        text = \"\\t\".join([x for x in mafDict[mafDict.keys()[0]].keys()]) + \"\\n\"\n",
    "        o.write(\"%s\" % text)\n",
    "        print text\n",
    "        count = 0\n",
    "        for locus in mafDict.keys():\n",
    "            text = locus + \"\\t\" + \"\\t\".join([str(x) for x in mafDict[locus].values()]) + \"\\n\"\n",
    "            o.write(\"%s\" % text)\n",
    "            count += 1\n",
    "else:\n",
    "    print \"pop-level maf already created, skipping calculation\"\n",
    "\n",
    "print \"reading pop-level maf\"\n",
    "impMAF = pd.read_csv(filE,header=0,index_col=0,sep=\"\\t\")\n",
    "##############################################################################################\n",
    "\n",
    "#get outlier snps\n",
    "print \"getting outlier snps\"\n",
    "df = pd.read_csv(op.join(d,'OutFLANK_snps.txt'),header=0,sep=\"\\t\")  \n",
    "outliersnps = df[\"x\"].tolist()\n",
    "\n",
    "#do pairwise to get Dij for outliers\n",
    "print \"getting pop counts\"\n",
    "popcounts = OrderedDict()\n",
    "for pop in pops:\n",
    "    popcounts[pop] = len(popDict[pop])\n",
    "\n",
    "print \"getting dij for outliers\"\n",
    "dijDict = OrderedDict() \n",
    "icount = 0\n",
    "for i,locusi in enumerate(outliersnps):\n",
    "    dijDict[locusi] = OrderedDict()\n",
    "    qi = glob[locusi] #global maf\n",
    "    \n",
    "    for j,locusj in enumerate(outliersnps):\n",
    "        if i > j: #i=row, j=col : lower triangle \n",
    "            qj = glob[locusj] #global maf\n",
    "            \n",
    "            sums = 0\n",
    "            for pop in impMAF.columns:\n",
    "                qik = impMAF.loc[locusi,pop] #get pop maf\n",
    "                qjk = impMAF.loc[locusj,pop] #get pop maf\n",
    "                nk = popcounts[pop]\n",
    "                \n",
    "                sums += (nk/244)*((qik*qjk)-(qi*qj))\n",
    "\n",
    "            dijDict[locusi][locusj] = sums\n",
    "        else:\n",
    "            dijDict[locusi][locusj] = np.nan\n",
    "    icount += 1\n",
    "    if icount % 10 == 0:\n",
    "        print icount\n",
    "#write out the file\n",
    "print \"writing dij for outliers\"\n",
    "rowcount = 0\n",
    "filE = op.join(d,'imputed_dvals.txt')\n",
    "with open(filE,\"w\") as o:\n",
    "    key0 = dijDict.keys()[0]\n",
    "    line = \"\\t\".join(dijDict[key0].keys()) + str(\"\\n\")\n",
    "    o.write(\"%s\" % line)\n",
    "    for locusi in dijDict.keys():\n",
    "        line = str(locusi)+\"\\t\"+\"\\t\".join([str(x) for x in dijDict[locusi].values()]) + str(\"\\n\")\n",
    "        o.write(\"%s\" % line)\n",
    "dvals = pd.read_csv(filE,header=0,index_col=0,sep=\"\\t\")\n",
    "\n",
    "#get global expected heterozygosity and bins\n",
    "print \"getting global het bins\"\n",
    "filE = \"/home/lindb/wbp/OutFLANK/Hexp_by_snp_withbins.txt\"\n",
    "H = pd.read_csv(filE, header=0,index_col=0,sep=\"\\t\")\n",
    "H.index = [snp for snp in H[\"locus\"].tolist()]\n",
    "\n",
    "#get a dataframe with the outlier loci and their bins\n",
    "print \"getting bins for outlier data\"\n",
    "outlierdata = pd.DataFrame(H[H[\"locus\"].isin(outliersnps)])\n",
    "outlierdata.index = [snp for snp in outlierdata[\"locus\"].tolist()]\n",
    "\n",
    "print \"determining non-outliers for null draws\"\n",
    "nonsigs = set(H.index.tolist()) - set(outlierdata.index.tolist())\n",
    "nonsigs = [x for x in nonsigs]\n",
    "nonsigdata = pd.DataFrame(H[H[\"locus\"].isin(nonsigs)])\n",
    "\n",
    "#how many random snps from each bin?\n",
    "print \"creating binCounter for outlier loci\"\n",
    "binCounter = Counter()\n",
    "for row in outlierdata.index:\n",
    "    binCounter[outlierdata.loc[row,\"bin\"]] += 1\n",
    "\n",
    "folder = op.join(d,'covariances/randmatrices/randsnps')\n",
    "if not op.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "#make 1000 dataframes of random snps of equal size\n",
    "print \"making 1000 dfs of random snps\"\n",
    "for i in range(1000):\n",
    "    snps = []        \n",
    "    for binn in binCounter.keys():\n",
    "        data = nonsigdata[nonsigdata[\"bin\"] == binn]\n",
    "\n",
    "        [snps.append(snp) for snp in random.sample(data.index,binCounter[binn])]\n",
    "\n",
    "    filE = op.join(folder,\"outflank_null_%s_randsnps.txt\" % (str(i).zfill(4)))\n",
    "    df = pd.DataFrame(snps)\n",
    "    df.to_csv(filE,header=False,index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/outflank_null/andrews/'\n",
    "for d in [DIR]:\n",
    "    DIR = op.join(d, 'covariances/randmatrices/randsnps/')\n",
    "    files = [op.join(DIR,f) for f in ls(DIR)]\n",
    "    assert len(files) == 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = '/home/lindb/wbp/outflank_null/andrews/'\n",
    "\n",
    "#get global minor allele frequencies\n",
    "print \"getting global maf\"\n",
    "glob = pickle.load(open(\"/home/lindb/wbp/OutFLANK/global_mafs.pkl\",\"rb\"))\n",
    "\n",
    "#get 012 data with population assignments\n",
    "print \"uploading 012\"\n",
    "filE = op.join(d,'SNPmat_HEADERIDX.pkl')\n",
    "data = pickle.load(open(filE,\"rb\"))\n",
    "\n",
    "print \"merging data frames\"\n",
    "m = pd.read_csv(op.join(d,'SNPmat_popNames.txt'), header=None,index_col=None,sep=\"\\t\")    \n",
    "data[\"pop\"] = m[0].tolist()\n",
    "cols = [\"pop\"] + [col for col in data.columns if \"NODE\" in col]\n",
    "data = data[cols]\n",
    "\n",
    "#get a list of samps for each population\n",
    "print \"getting samp lists per pop - these samps have been randomly assigned\"\n",
    "pops = np.unique(data[\"pop\"]).tolist()\n",
    "popDict = OrderedDict()\n",
    "for samp in data.index:\n",
    "    pop = data.loc[samp,\"pop\"]\n",
    "    if not pop in popDict.keys():\n",
    "        popDict[pop] = []\n",
    "    popDict[pop].append(samp)\n",
    "\n",
    "print \"getting pop counts\"\n",
    "popcounts = OrderedDict()\n",
    "for pop in pops:\n",
    "    popcounts[pop] = len(popDict[pop])\n",
    "    \n",
    "popMAF = pd.read_csv(op.join(d,'imputed_MAF.txt'),header=0,index_col=0,sep=\"\\t\")    \n",
    "\n",
    "print \"calculating dij for 1000 sets of null snps\"\n",
    "DIR = op.join(d,'covariances/randmatrices/randsnps/')\n",
    "files = [op.join(DIR,f) for f in ls(DIR)]\n",
    "assert len(files) == 1000\n",
    "COUNTS = 0\n",
    "for f in sorted(files):\n",
    "    num = op.basename(f).split(\"_\")[2]\n",
    "    df = pd.read_csv(f, header=None,sep=\"\\t\")\n",
    "    randomsnps = df[0].tolist()\n",
    "\n",
    "    dijDict = OrderedDict() \n",
    "    for i,locusi in enumerate(randomsnps):\n",
    "        dijDict[locusi] = OrderedDict()\n",
    "        qi = glob[locusi] #global maf\n",
    "\n",
    "        for j,locusj in enumerate(randomsnps):\n",
    "            if i > j: #i=row, j=col : lower triangle \n",
    "                qj = glob[locusj] #global maf\n",
    "\n",
    "                sums = 0\n",
    "                for pop in popMAF.columns:\n",
    "                    qik = popMAF.loc[locusi,pop] #get pop maf\n",
    "                    qjk = popMAF.loc[locusj,pop] #get pop maf\n",
    "                    nk = popcounts[pop]\n",
    "\n",
    "                    sums += (nk/244)*((qik*qjk)-(qi*qj))\n",
    "\n",
    "                dijDict[locusi][locusj] = sums\n",
    "            else:\n",
    "                dijDict[locusi][locusj] = np.nan\n",
    "    DIR = op.join(d,'covariances/randmatrices/randdij/')\n",
    "    if not op.exists(DIR):\n",
    "        os.makedirs(DIR)\n",
    "\n",
    "    filE = op.join(DIR,\"outflank_null_andrew_%s_DVALS.txt\" % num)   \n",
    "    with open(filE,'w') as o:\n",
    "        key0 = dijDict.keys()[0]\n",
    "        line = '\\t'.join(dijDict[key0].keys()) + str('\\n')\n",
    "        o.write(\"%s\" % line)\n",
    "        for locusi in dijDict.keys():\n",
    "            line = str(locusi)+'\\t'+'\\t'.join([str(x) for x in dijDict[locusi].values()]) + str('\\n')\n",
    "            o.write(\"%s\" % line)\n",
    "    COUNTS += 1\n",
    "    if COUNTS % 10 == 0:\n",
    "        print COUNTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = '/home/lindb/wbp/outflank_null/andrews'\n",
    "\n",
    "print \"getting empirical dij value\"\n",
    "filE = op.join(d,'imputed_dvals.txt')\n",
    "print filE\n",
    "DF = pd.read_csv(filE,header=0,index_col=0,sep=\"\\t\")\n",
    "empdijs = []\n",
    "for i,locusi in enumerate(DF.index):\n",
    "    for j,locusj in enumerate(DF.columns):\n",
    "        if i > j:\n",
    "            empdijs.append(abs(float(DF.loc[locusi,locusj])))\n",
    "empmed = np.median(empdijs)\n",
    "\n",
    "DIR = op.join(d,'covariances/randmatrices/randdij')\n",
    "files = [op.join(DIR,f) for f in ls(DIR)]\n",
    "assert len(files) == 1000\n",
    "randmeds = []\n",
    "for f in sorted(files):\n",
    "    df = pd.read_csv(f,header=0,index_col=0,sep=\"\\t\")\n",
    "    vals = []\n",
    "    for i,row in enumerate(df.index):\n",
    "        for j,col in enumerate(df.columns):\n",
    "            if i > j:\n",
    "                vals.append(abs(float(df.loc[row,col])))\n",
    "    med = np.median(vals)\n",
    "    randmeds.append(med)\n",
    "\n",
    "for i,val in enumerate(sorted(randmeds)):\n",
    "    if empmed > val:\n",
    "        perc = (i+1)/len(randmeds)\n",
    "        \n",
    "dataf = pd.DataFrame()\n",
    "dataf['randmeds'] = sorted(randmeds)\n",
    "dataf.loc[0,'empmed'] = empmed\n",
    "dataf.loc[0,'percentile'] = perc\n",
    "dataf.loc[0,'greater_than_max'] = empmed / max(randmeds)\n",
    "dataf.loc[0,'greater_than_n5th'] = empmed / sorted(randmeds)[int(round(len(randmeds)*0.95))]\n",
    "print len(DF.index)\n",
    "#dataf.loc[0,'len_outliers'] = len(DF.index)\n",
    "\n",
    "filE = op.join(d,'results.txt')\n",
    "dataf.to_csv(filE,header=True,index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
